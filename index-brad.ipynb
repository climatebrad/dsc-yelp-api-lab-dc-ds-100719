{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp API - Lab\n",
    "\n",
    "\n",
    "## Introduction \n",
    "\n",
    "We've seen how the Yelp API works and how to create basic visualizations using Folium. It's time to put those skills to work in order to create a working map! Taking things a step further, you'll also independently explore how to perform pagination in order to retrieve a full results set from the Yelp API!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to: \n",
    "\n",
    "* Using pagination and multiple functions, gather large amounts of data from an API, parse the data and make sense of it with meaningful analysis\n",
    "* Create maps using Folium\n",
    "\n",
    "## Problem Introduction\n",
    "\n",
    "You've now worked with some API calls, but we have yet to see how to retrieve a more complete dataset in a programmatic manner. Returning to the Yelp API, the [documentation](https://www.yelp.com/developers/documentation/v3/business_search) also provides us details regarding the API limits. These often include details about the number of requests a user is allowed to make within a specified time limit and the maximum number of results to be returned. In this case, we are told that any request has a maximum of 50 results per request and defaults to 20. Furthermore, any search will be limited to a total of 1000 results. To retrieve all 1000 of these results, we would have to page through the results piece by piece, retrieving 50 at a time. Processes such as these are often referred to as pagination.\n",
    "\n",
    "In this lab, you will define a search and then paginate over the results to retrieve all of the results. You'll then parse these responses as a DataFrame (for further exploration) and create a map using Folium to visualize the results geographically.\n",
    "\n",
    "## Part I - Make the Initial Request\n",
    "\n",
    "Start by making an initial request to the Yelp API. Your search must include at least 2 parameters: **term** and **location**. For example, you might search for pizza restaurants in NYC. The term and location is up to you but make the request below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from markdown import markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "#Our previous function for loading our api key file\n",
    "def get_keys(path,api=None):\n",
    "    with open(path) as f:\n",
    "        if api:\n",
    "            return json.load(f)[api]\n",
    "        else:\n",
    "            return json.load(f)\n",
    "\n",
    "keys = get_keys(os.environ['HOME']+\"/.secret/keys.json\",'yelp')\n",
    "\n",
    "api_key = keys['api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "headers = {\n",
    "        'Authorization': 'Bearer {}'.format(api_key),\n",
    "    }\n",
    "\n",
    "url_params = {\n",
    "                'location': 'DC',\n",
    "                'limit' : 50,\n",
    "                'categories': 'bookstores,usedbooks'\n",
    "            }\n",
    "response = requests.get(url, headers=headers, params=url_params)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagination\n",
    "\n",
    "Now that you have an initial response, you can examine the contents of the JSON container. For example, you might start with ```response.json().keys()```. Here, you'll see a key for `'total'`, which tells you the full number of matching results given your query parameters. Write a loop (or ideally a function) which then makes successive API calls using the offset parameter to retrieve all of the results (or 5000 for a particularly large result set) for the original query. As you do this, be mindful of how you store the data. Your final goal will be to reformat the data concerning the businesses themselves into a pandas DataFrame from the json objects.\n",
    "\n",
    "**Note: be mindful of the API rate limits. You can only make 5000 requests per day and are also can make requests too fast. Start prototyping small before running a loop that could be faulty. You can also use time.sleep(n) to add delays. For more details see https://www.yelp.com/developers/documentation/v3/rate_limiting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here; use a function or loop to retrieve all the results from your original request\n",
    "import time\n",
    "\n",
    "url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "headers = {\n",
    "        'Authorization': 'Bearer {}'.format(api_key),\n",
    "    }\n",
    "\n",
    "limit = 50\n",
    "url_params = {\n",
    "                'location': 'DC',\n",
    "                'limit' : limit,\n",
    "                'categories': 'bookstores'\n",
    "            }\n",
    "\n",
    "businesses = response.json()['businesses']\n",
    "total = response.json()['total']\n",
    "offset = 0\n",
    "max_request = max(total,4999)\n",
    "attempts = 0\n",
    "max_attempts = total // limit + 1\n",
    "while (len(businesses) < max_request) and (attempts < max_attempts):\n",
    "    url_params['offset'] = len(businesses)\n",
    "    response = requests.get(url, headers=headers, params=url_params)    \n",
    "    businesses.extend(response.json()['businesses'])\n",
    "    attempts += 1\n",
    "\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use pandas' json_normalize functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.io.json.json_normalize(businesses).set_index('id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_val'] = df.price.map(len,na_action='ignore')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a side dataframe with the categories broken out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = pd.io.json.json_normalize(businesses, 'categories',['id']).rename(columns={'id':'business_id'})\n",
    "cat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "\n",
    "Take the restaurants from the previous question and do an initial exploratory analysis. At minimum, this should include looking at the distribution of features such as price, rating and number of reviews as well as the relations between these dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df[['distance','rating','review_count','price_val']]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping\n",
    "\n",
    "Look at the initial Yelp example and try and make a map using Folium of the restaurants you retrieved. Be sure to also add popups to the markers giving some basic information such as name, rating and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_map = folium.Map([df['coordinates.latitude'].mean(), df['coordinates.longitude'].mean()], zoom_start=13)\n",
    "base_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rating.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating_bin'] = pd.cut(df.rating, 6, labels=False, retbins=False, right=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [ 'darkred', 'red', 'orange', 'lightblue', 'blue',  'darkblue']\n",
    "\n",
    "base_map = folium.Map([df['coordinates.latitude'].mean(), df['coordinates.longitude'].mean()], zoom_start=13)\n",
    "\n",
    "\n",
    "def row_to_marker(row, map = base_map):\n",
    "    lat = row['coordinates.latitude']\n",
    "    long = row['coordinates.longitude']\n",
    "    color = colors[row.rating_bin]\n",
    "    popup_text = f\"**{row['name']}**  \\nrating: {row.rating}\"\n",
    "    if not pd.isna(row['price']):\n",
    "        popup_text += f\"  \\nprice: {row['price']}\"\n",
    "    icon = folium.Icon(icon='map-marker-alt', color=color)\n",
    "    folium.Marker(location=[lat, long], popup=markdown(popup_text), icon=icon).add_to(base_map)\n",
    "    \n",
    "df.apply(row_to_marker, axis=1)\n",
    "base_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Nice work! In this lab, you've made multiple API calls to Yelp in order to paginate through a results set, performing some basic exploratory analysis and then creating a nice interactive map to display the results using Folium! Well done!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
